# ────────────────────────────────────────────────
#  DeepSeek‑R1‑671B · Fine‑Tuning Configuration
#  save as configs/training_config.yaml
# ────────────────────────────────────────────────

run:
  precision: bf16               # bf16 / fp16 / fp32
  seed: 42
  output_dir:  <<< edit >>>     # overridden by train_model.sh
  logging_dir: <<< edit >>>     # overridden by train_model.sh
  run_name:    <<< auto >>>     # set by CLI flag

data:
  train_path:    <<< edit: /data/processed/train.jsonl >>>
  eval_path:     <<< edit: /data/processed/valid.jsonl >>>
  dataset_format: jsonl          # supports jsonl / arrow / parquet
  text_key:      text            # column containing prompt+completion
  max_length:    8192            # token limit (8k sequence budget)
  num_workers:   8

model:
  base_model_name_or_path:  deepseek-ai/deepseek-llm-67b-chat
  # Alternative local path: /home/ubuntu/hf_models/deepseek-r1-671b
  trust_remote_code: true

training:
  strategy:      qlora           # full / lora / qlora
  batch_size:
    per_device_train: 1          # GPUs are large; keep small
    per_device_eval:  1
    gradient_accumulation: 8     # Effective global batch = 8
  epochs:         1              # Or use max_steps instead
  learning_rate:  1.5e-5
  lr_scheduler:   cosine
  warmup_ratio:   0.03
  weight_decay:   0.0
  max_grad_norm:  1.0
  gradient_checkpointing: true
  deepspeed_config:
    stage:          2            # ZeRO stage (1‑3)
    offload_optimizer: false
    bf16:           true
    enable_cpu_offload: false
    train_micro_batch_size_per_gpu: ${training.batch_size.per_device_train}
    gradient_accumulation_steps:   ${training.batch_size.gradient_accumulation}

lora:                               # used if strategy is lora/qlora
  r:               64
  alpha:           16
  dropout:         0.05
  target_modules:  [q_proj, k_proj, v_proj, o_proj, gate_proj, up_proj]

evaluation:
  log_interval:    50
  save_interval:   500
  eval_interval:   1000
  metrics:         [loss]

checkpointing:
  save_total_limit: 3
  save_strategy:   steps
  resume_from:     null           # path to checkpoint for resume

callbacks:
  early_stopping:
    enabled: false
    patience: 3
    threshold: 0.01
