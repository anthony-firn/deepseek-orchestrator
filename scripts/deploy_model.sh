#!/usr/bin/env bash
#
# scripts/deploy_model.sh
# -----------------------------------------------------------
# Spin up a vLLM server for a fineâ€‘tuned (or base) DeepSeek model.
#
#  Usage:
#     ./deploy_model.sh                        # serve latest checkpoint
#     MODEL_PATH=/path/to/alt_checkpoint ./deploy_model.sh
#     PORT=9000 BACKEND=gpu ./deploy_model.sh  # custom options
#
#  Environment variables (all optional):
#     MODEL_PATH  â€“ directory with model weights/tokenizer (default: latest ckpt)
#     PORT        â€“ port to expose (default 8000)
#     HOST        â€“ bind address (default 0.0.0.0)
#     BACKEND     â€“ "gpu" or "cpu" (default gpu)
#     MAX_BATCH   â€“ max batch size (default 16)
#
#  Logs stream to stdout; use systemd / tmux / nohup as desired.
# -----------------------------------------------------------
set -euo pipefail
IFS=$'\n\t'

PROJECT_NAME=${PROJECT_NAME:-"deepseek-orchestrator"}
EXP_DIR=${EXP_DIR:-"$HOME/${PROJECT_NAME}_experiments"}
PORT=${PORT:-8000}
HOST=${HOST:-"0.0.0.0"}
BACKEND=${BACKEND:-"gpu"}
MAX_BATCH=${MAX_BATCH:-16}

# Pick the latest checkpoint if MODEL_PATH not provided
if [[ -z "${MODEL_PATH:-}" ]]; then
  MODEL_PATH=$(ls -dt "$EXP_DIR"/*/checkpoints 2>/dev/null | head -n1 || true)
  if [[ -z "$MODEL_PATH" ]]; then
    echo "âŒ  No checkpoints found in $EXP_DIR â€“ specify MODEL_PATH explicitly."
    exit 1
  fi
fi

echo "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€"
echo "ðŸš€  vLLM Deployment"
echo "Model path  : $MODEL_PATH"
echo "Bind        : $HOST:$PORT"
echo "Backend     : $BACKEND"
echo "Max batch   : $MAX_BATCH"
echo "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€"
sleep 1

# â”€â”€ activate Conda env â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
source "$HOME/miniconda/etc/profile.d/conda.sh"
conda activate deepseek

# â”€â”€ install vllm if missing â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if ! python -c "import vllm" &>/dev/null; then
  echo "ðŸ”§  Installing vLLM ..."
  pip install --upgrade "vllm>=0.3.2"
fi

# â”€â”€ launch server â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
python -m vllm.entrypoints.openai.api_server \
  --model             "$MODEL_PATH" \
  --host              "$HOST" \
  --port              "$PORT" \
  --backend           "$BACKEND" \
  --max-batch-tokens  "$MAX_BATCH" \
  --download-dir      "$HOME/hf_models" \
  --tensor-parallel-size $(( $(nvidia-smi --query-gpu=name --format=csv,noheader | wc -l) )) \
  "$@"

# Notes:
# â€¢ --tensor-parallel-size autoâ€‘sets to GPU count; adjust for multiâ€‘node with vLLMâ€™s distributed launcher.
# â€¢ Add --trust-remote-code if your model repo requires custom code.
# â€¢ Pass extra CLI flags to vLLM by appending them after "$@".

